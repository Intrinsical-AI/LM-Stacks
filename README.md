# LLM Stacks

> Structured repository to document patterns, experiments, and learnings about Large Language Models (LLMs)

## ğŸ“ Repository layout

```text
.
â”œâ”€â”€ prompting/              # Prompt templates and experiments
â”‚   â””â”€â”€ templates/          # Parameterised prompt templates
â”œâ”€â”€ stacks/                 # Technology stacks for different problem domains
â”œâ”€â”€ techniques/             # Methodologies for working with LLMs (TDD, Multi-Agent, Live-RAG, â€¦)
â”‚   â”œâ”€â”€ Live-RAG/
â”‚   â”œâ”€â”€ MultiAgent/
â”‚   â””â”€â”€ TestDrivenDevelopment/
â””â”€â”€ LICENSE                 # MIT license
```

## ğŸš€ Getting started

1. Clone the repo and create a Python virtual environment (optional, but usefull in general if you havenâ€™t try it already)
2. Explore the folders above to find ready-to-use templates, stacks, or techniques relevant to your project.
3. Contribute back! Feel free to open pull-requests with new templates, methodologies, or improvements.

## ğŸ“š What you will find here

* **Prompting templates** â€“ reusable prompt skeletons designed for chats, code generation, and evaluation.
* **Techniques** â€“ step-by-step guides that explain how to combine LLMs and humans effectively (TDD, Multi-Agent, Live-RAG â€¦).
* **Stacks** â€“ opinionated selections of libraries and tools for tasks such as data analysis, RAG, or UI prototyping.

## ğŸ—ºï¸ Roadmap

- Add unit & integration test examples for each technique.
- Expand the `stacks` section with containerised starter projects.
- Provide reference implementations for automated evaluation pipelines.

## ğŸ“ License

This project is licensed under the MIT License â€“ see the `LICENSE` file for details.

---

Feel free to raise issues if you spot inconsistencies or have suggestions. Happy hacking! ğŸš€
